---
title: Model assessment and variable selection
subtitle: Making simpler model for complex analysis
slug: model-assessment-variable-selection
author: TheRimalaya
code-fold: true
date: 2017-03-05
date-modified: 2024-10-08
toc: true
tags:
  - Statistics
  - Modeling
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  comment = NA,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.asp = 0.5,
  out.width = "100%",
  results = "hold"
)
library(gt)
library(tidytable)
library(ggplot2)
```

Adding new variables to a model can introduce noise, complicating analysis. Simpler models tend to be better because they’re easier to understand and contain less noise. Statistical methods can help us choose the best variables and improve our models.

This tutorial will show you how to compare models to find the best one. Using the mtcars dataset available in R, it will explore methods for variable selection that help build efficient models with fewer variables. Here are two popular techniques for selecting the best subset of variables:

# Best subset method

The best subset selection procedure identifies the optimal regression model by evaluating all possible subsets of variables. As the number of variables increases, the number of potential combinations grows exponentially. For instance, with 2 variables, there are 4 possible subsets: one with no predictors, two with single predictors, and one with both predictors. With 4 variables, the number of possible subsets jumps to 16. Consequently, fitting all possible subsets of a large number of predictors can become computationally intensive.

After fitting all possible models, they are compared based on various criteria. These criteria may include the coefficient of determination ($R^2$), adjusted $R^2$, Mallow’s CP, AIC, and BIC. In R, we can use the [leaps](https://www.rdocumentation.org/packages/leaps/versions/2.1-1) package for this task. Let's look at the `mtcars` example to see this in action.

## Complete/full model

```{r}
#| label: full-model-fit
#| code-summary: Fitting a complete model
full.model <- lm(mpg ~ ., data = mtcars)
smry <- summary(full.model)
smry
```

Here, we can see that the model has explained almost `r round(smry [['r.squared']]*100, 2)` percent of variation present in `mpg`, but non of the predictors are significant. This is a hint of having unnecessary variables that has increased model error. Using `regsubsets` function from `leaps` package, we can select a subset of predictors based on some criteria.

## Selecting best subset
```{r}
#| label: best-subset
#| code-summary: Selecting best subset model
library(leaps)
best.subset <- regsubsets(
  x      = mtcars[, -1], # predictor variables
  y      = mtcars[, 1], # response variable (mpg)
  nbest  = 1, # top 1 best model
  nvmax  = ncol(mtcars) - 1, # max. number of variable (all)
  method = "exhaustive" # search all possible subset
)
bs.smry <- summary(best.subset)
```

We can combine following summary output with a plot created from additional estimates to get some insight. These estimates are also found in the summary object. The output show which variables are included with a star(`*`). 

```{r}
#| label: best-subset-out
#| code-summary: Summary of best subset
bs.smry$outmat %>% 
  as_tidytable(.keep_rownames = "model") %>% 
  gt::gt(rowname_col = "model") %>% 
  gt::opt_vertical_padding(0.5) %>% 
  gt::opt_row_striping() %>% 
  gt::tab_options(
    column_labels.font.weight = "bold",
    stub.font.weight = "bold"
  )

bs.est <- tidytable(
  nvar   = 1:(best.subset$nvmax - 1),
  adj.r2 = round(bs.smry$adjr2, 3),
  cp     = round(bs.smry$cp, 3),
  bic    = round(bs.smry$bic, 3)
) %>% pivot_longer(
  cols = c(adj.r2:bic),
  names_to = "estimates",
  values_to = "value"
)
```

We can make a plot to visualise the properties of these individual models and select a model with specific number of predictor that can give minimum BIC, or minimum CP or maximum adjusted rsquared.

```{r}
#| label: plt-best-subset
#| fig-cap: Adj-Rsq, BIC, and Mallows' CP for models with increasing number of predictors
#| code-summary: Plotting Adj-Rsq, BIC, and CP
#| message: false
#| warning: false
bs.est.select <- bs.est %>%
  group_by(estimates) %>%
  filter(
    (value == max(value) & estimates == "adj.r2") |
      (value == min(value) & estimates != "adj.r2")
  )
ggplot(bs.est, aes(nvar, value, color = estimates)) +
  geom_point(shape = 21, fill = "lightgray") +
  geom_line() +
  facet_wrap(~estimates, scale = "free_y") +
  theme(legend.position = "top") +
  labs(
    x = "Number of variables in the model",
    y = "Value of Estimate"
  ) +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  geom_point(
    data = bs.est.select, fill = "red",
    shape = 21
  ) +
  geom_text(
    aes(label = paste0("nvar:", nvar, "\n", "value:", value)),
    data = bs.est.select,
    size = 3, hjust = 0, vjust = c(1, -1, -1),
    color = "black", family = "monospace"
  )
```

From these plots, we see that with 5 variables we will obtain maximum adjusted coefficient of determination ($R^2$). Similarly, both BIC and Mallow CP will be minimum for models with only 3 predictor variables. With the help of table above, we can identify these variables. From the table, row corresponding to 3 variables, we see that the three predictors are `wt`, `qsec` and `am`. To obtain maximum adjusted $R^2$, `disp` and `hp` should be added to the previous 3 predictors.

This way, we can reduce a model to few variables optimising different assessment criteria. Let look at the fit of these reduced models:

```{r}
#| label: reduced-model
#| code-summary: Reduced Model
model.3 <- lm(mpg ~ wt + qsec + am, data = mtcars)
model.5 <- update(model.3, . ~ . + disp + hp)
```

## Model summaries
::: panel-tabset

### 3 Variable Model
```{r}
#| label: model.3-summary
#| code-summary: Model summary of model with 3 variables
summary(model.3)
```

### 5 Variable Model
```{r}
#| label: model.5-summary
#| code-summary: Model summary of model with 5 variables
summary(model.5)
```

:::

From these output, it seems that although adjusted $R^2$ has increased in later model, the additional variables are not significant. we can compare these two model with an ANOVA test which compares the residual variance between these two models. We can write the hypothesis as,

$H_0:$ _Model 1_ and _Model 2_ are same vs $H_1:$ _Model 1_ and _Model 2_ are different

where, _Model 1_ and _Model 2_ represents 3 variable and 5 variable model

```{r}
#| label: anova
#| code-summary: Anova comparing Model with 3 and 5 variables
anova(model.3, model.5)
```

The ANOVA result can not reject the hypothesis so claim that _Model 1_ and _Model 2_ are same. So, it is better to select the simpler model with 3 predictor variables. 

# Step-wise selection #

In this section, we'll explore another type of variable selection method, similar yet distinct from best subset selection method. We will explore forward and backward stepwise selection methods using the `mtcars` dataset in R. We'll briefly compare these methods and provide insights into their application.

::: panel-tabset

## Forward Selection

Forward selection starts with an empty model, and predictors are added one by one based on a selection criterion, typically the Akaike Information Criterion (AIC).

```{r}
#| label: forward-selection
#| code-summary: Forward selection

# Initial empty model
initial_model <- lm(mpg ~ 1, data = mtcars)

# Full model with all predictors
full_model <- lm(mpg ~ ., data = mtcars)

# Stepwise forward selection
forward_model <- MASS::stepAIC(
  initial_model, 
  direction = "forward", 
  scope = list(lower = initial_model, upper = full_model)
)

# Summary of the forward selection model
summary(forward_model)
```

## Backward Selection

Backward selection begins with the full model, removing one predictor at a time to improve the model according to the chosen criterion, typically AIC.

```{r}
#| label: backward-selection
#| code-summary: Backward selection

# Full model with all predictors
full_model <- lm(mpg ~ ., data = mtcars)

# Stepwise backward selection
backward_model <- MASS::stepAIC(
  full_model, 
  direction = "backward"
)

# Summary of the backward selection model
summary(backward_model)
```

:::

# Comparing best subset and stepwise selection method

Best subset selection considers all possible combinations of predictors and selects the model with the best performance based on a specified criterion (e.g., AIC, BIC, adjusted R²). While comprehensive, this method can be computationally expensive for datasets with a large number of predictors.

- **Best Subset Selection**: Evaluates every possible model, providing the optimal model based on the criterion. It ensures the best fit but at a high computational cost.
- **Stepwise Selection (Forward and Backward)**: Evaluates models sequentially. It's more computationally efficient but may not guarantee the globally optimal model.

Stepwise selection methods, whether forward or backward, provide a practical approach to model selection by adding or removing predictors based on defined criteria. While not as exhaustive as best subset selection, they offer a balance between computational efficiency and model performance. By understanding and applying these methods within the `mtcars` dataset, we can navigate model selection systematically and efficiently.

## Glossary

1. **[R-squared (R²)](https://en.wikipedia.org/wiki/Coefficient_of_determination)**: A statistical measure representing the proportion of the variance for the dependent variable that's explained by the independent variables in the model. R² values range from 0 to 1, with higher values indicating better model performance.

2. **[Adjusted R-squared (R² adjusted)](https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2)**: Adjusted R² modifies R² to account for the number of predictors in the model. It provides a more accurate measure when comparing models with a different number of predictors, as it penalizes the addition of non-informative predictors.

3. **[Akaike Information Criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion)**: AIC is an estimator of the relative quality of statistical models for a given set of data. It balances the goodness of fit of the model with the number of predictors, penalizing more complex models to avoid overfitting. Lower AIC values indicate better models.

4. **[Bayesian Information Criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion)**: Similar to AIC, BIC also evaluates model quality but imposes a more substantial penalty for the number of predictors. BIC is used to discourage overfitting, and lower BIC values indicate a better model.

5. **[Mallow's CP](https://en.wikipedia.org/wiki/Mallows%27_Cp)**: Mallow's CP criterion assesses the trade-off between model complexity and goodness of fit. Lower values of CP are desired, with CP values close to the number of predictors plus one indicating well-fitted models.

---
title: "Simulating data for ANOVA similar to existing dataset for analysis"
author: TheRimalaya
date: "2023-02-04"
date-modified: "2024-10-21"
code-fold: true
categories:
  - Simulation
  - Statistics
tags:
  - Simulation
  - Statistics
  - ANOVA
---

Simulating data is an important tool in both education and research, and it has been extremely helpful for testing, comparing, and understanding concepts in practical and applied settings.


```{r, include = FALSE}
knitr::opts_chunk$set(
  comment = NA,
  out.width = "100%",
  dev = "svg"
)
library(tidytable)
library(ggplot2)

ggplot2::theme_set(ggthemes::theme_few() + theme(
  panel.grid = element_line(color = "#f0f0f0")
))

set.seed(124)
```

Often, we use Analysis of Variance (ANOVA) to analyze variances to find out if different cases result in similar outcomes and if the differences are significant. Some simple examples include:

- The effect of different diets on the growth of fish
- Comparing the heights of three different plant species
- The type of flour used for baking bread

These are common examples where, in some cases, data are collected by setting up an experiment, and in other cases, they are collected through sampling. This article explains how ANOVA analyzes the variance and in what situations are they significant through both simulated and real data.

Consider the following model with $i=3$ groups and $j=n$ observations,

$$
y_{ij} = \mu + \tau_i + \varepsilon_{ij}, \; i = 1, 2, 3 
\texttt{ and } j = 1, 2, \ldots n
$$

Here, $\tau_i$ is the effect corresponding to group $i$ and 
$\varepsilon_{ij} \sim \mathrm{N}(0, \sigma^2)$, the usual assumption of 
linear model. The simulation example below describes it in detail.

# Simulation Example

In this simulation example, I aim to replicate specific elements of the `USArrests` dataset by simulating 50 cases for three types of crimes: Murder, Assault, and Rape. For each crime, I categorize individuals into three groups based on illiteracy levels and simulate their respective arrest rates. Here’s concise overview of the process and analysis steps:

## Simulation Design

```{r}
#| code-summary: Simulation design
sim_design <- tidytable(
  Illiteracy = factor(
    rep(c(1, 2, 3), 3),
    labels = c("high", "medium", "low")
  ),
  Crime = factor(
    rep(c(1, 2, 3), each = 3),
    labels = c("Murder", "Assault", "Rape")
  ),
  mean = c(11, 8, 5, 214, 190, 114, 23, 21, 19),
  sd = c(3, 4, 3, 79, 82, 55, 8, 10, 10)
)

sim_design
```

Since these data cannot contain negative values so instead of using `rnorm` available in `stats` package, I will use `truncnorm` available in GitHub.  There are other options as well which can be used such as: ...

If not installed, install the package as 
`remotes::install_github("olafmersmann/truncnorm")` or 
`devtools::install_github("olafmersmann/truncnorm")`.

### Code for Simulation, Analysis, and Plot

Let's simulate 50 observation Arrest `Rate` in each levels of `Illiteracy`, and `Crime` in simulation design.

```{r}
#| code-summary: Number of simulation
#| code-fold: false
nsim <- 50
```

::: panel-tabset

### Simulation Code

```{r}
#| code-summary: Simulated data
#| code-fold: false
sim_data <- sim_design %>% 
  group_by(Illiteracy, Crime) %>% 
  mutate(rate = map2(mean, sd, ~tidytable(
    Rate = truncnorm::rtruncnorm(
      n = nsim, a = 0, b = Inf, mean = .x, sd = .y
    ) %>% round()
  ))) %>% 
  unnest() %>% ungroup() %>% 
  nest(.by = c(Crime))
```

Here, Arrest rates were generated from a normal distribution using `truncnorm` from `truncnorm` package to get only positive value and also mirroring the mean and standard deviation in `USArrests`. Using normal distributions ensures the synthetic data mimics the actual data’s variation and mean, making the simulations realistic.

### Data from simulation

```{r}
#| code-summary: Simulated data by group
sim_data
```

::: panel-tabset

#### Murder

```{r}
#| code-summary: Simulated data for Murder
head(sim_data[Crime == "Murder", data][[1]])
```

#### Assault

```{r}
#| code-summary: Simulated data for Assault
head(sim_data[Crime == "Assault", data][[1]])
```

#### Rape

```{r}
#| code-summary: Simulated data for Rape
head(sim_data[Crime == "Rape", data][[1]])
```


:::

:::

Using the simulated data above, we now fit an Anova model with `Illiteracy` as a factor (group) variable that affects the Arrest `Rate` (response variable) separately for each `Crime`. I have also made a density plot for the `Rate` variable for both simulated data and plot it with normal curve with corresponding mean and standard deviation. Following are the codes for fitting the Anova model, and creating density plot and box plot. Also we will perform a Posthoc test using Tukey's method to make a pairwise comparison of different `Illiteracy` levels.

::: {.callout-tip collapse="true"}
## Code for Model Fit and Plotting

::: panel-tabset

### Model and plot data
```{r}
#| code-summary: Model fit and prepare data for plot
#| code-fold: false
mdl_fit <- sim_data %>% 
  mutate(
    Fit = map(data, ~lm(Rate ~ Illiteracy, data = .x)),
    Summary = map(Fit, summary),
    Anova = map(Fit, anova),
    Tukey = map(Fit, aov) %>% map(TukeyHSD)
  )

mdl_est <- mdl_fit %>% 
  summarize(
    across(Summary, map, broom::tidy), 
    .by = c(Crime)
  ) %>% unnest()

mdl_fit_df <- mdl_fit %>% 
  summarize(
    across(Fit, map, broom::augment),
    .by = c(Crime)
  ) %>% unnest()

eff_df <- mdl_fit %>% 
  summarize(
    across(Fit, map, function(.fit) {
      effects::Effect("Illiteracy", .fit) %>%
        as_tidytable()
    }),
    .by = "Crime"
  ) %>% unnest()

tky_df <- mdl_fit %>% 
  summarize(
    across(Tukey, function(tky) {
      map(tky, purrr::pluck, "Illiteracy") %>%
        map(as_tidytable, .keep_rownames = "terms")
    }),
    .by = "Crime"
  ) %>% unnest()
```

### Density plot
```{r}
#| code-summary: Density plot
#| code-fold: false
density_plot <- sim_data %>% 
  unnest(data) %>% 
  ggplot(aes(Rate, color = Illiteracy)) +
    facet_wrap(
      facets = vars(Crime),
      ncol = 3,
      scales = "free",
    ) +
    geom_density(aes(linetype = "Simulated")) +
    geom_density(
      aes(linetype = "Fitted", x = .fitted),
      data = mdl_fit_df
    ) +
    geom_rug(
      data = eff_df,
      aes(x = fit)
    ) +
    scale_linetype_manual(
      breaks = c("Simulated", "Fitted"),
      values = c("solid", "dashed")
    ) +
    scale_color_brewer(palette = "Set1") +
    theme(legend.position = "bottom") +
    labs(
      x = "Arrest Rate",
      y = "Density",
      linetype = NULL
    )
```

### Box plot
```{r}
#| code-summary: Boxplot
#| code-fold: false
effect_plot <- mdl_fit_df %>% 
  ggplot(aes(Rate, Illiteracy)) +
    facet_grid(
      cols = vars(Crime),
      scales = "free_x"
    ) +
    geom_boxplot(
      notch = TRUE, 
      color = "grey",
      outlier.colour = "grey"
    ) +
    geom_point(
      position = position_jitter(height = 0.25),
      color = "grey",
      size = rel(0.9)
    ) +
    geom_pointrange(
      aes(
        color = "Estimated",
        xmin = lower,
        xmax = upper,
        x = fit
      ),
      data = eff_df
    ) +
    geom_point(
      aes(color = "True Mean", x = mean),
      data = sim_design
    ) +
    scale_color_brewer(
      name = "Mean",
      palette = "Set1"
    ) +
    theme(
      legend.position = "bottom"
    )
```

### Posthoc plot
```{r}
#| code-summary: Post-hoc Tukey plot
#| code-fold: false
tukey_plot <- tky_df %>% 
  ggplot(aes(diff, terms)) +
    facet_grid(
      cols = vars(Crime), 
      scales = "free_x"
    ) +
    geom_pointrange(
      aes(xmin = lwr, xmax = upr, x = diff),
      shape = 21,
      fill = "whitesmoke"
    ) +
    geom_vline(
      xintercept = 0,
      linetype = "dashed",
      color = "royalblue"
    ) +
    scale_color_brewer(
      name = "Mean",
      palette = "Set1"
    ) +
    labs(
      y = "Illiteracy",
      x = "Effect difference",
      title = "Pairwise comparison of levels of illitracy"
    ) +
    expand_limits(x = 0)
```

:::

:::

## Analysis

::: panel-tabset

### Distribution

```{r}
#| code-summary: Density of simulated data
#| fig-asp: 0.5
density_plot
```

Here, the kernel density plots for arrest rates were shown alongside the normal density curve. This visual assessment checked the goodness of fit between simulated data and the expected normal distribution. The close match between kernel density and normal density validates that the data follows a normal distribution, confirming the simulation's accuracy.

### Model Fit

A one-way ANOVA output below helps to find if there is any difference between arrest rate based on illiteracy level for each crime. Here we see that in all crimes high illiteracy level was considered as reference and compared to this both medium and low illiteracy levels have lower arrest rate. This suggest that the higher illiteracy rate corresponds to higher arrest rate. However, for crimes: assault and rape, the effect of medium illiteracy rate has high p-value and can not be considered to have significant effect on arrest rate.

::: panel-tabset

#### Murder

```{r}
#| code-summary: "ANOVA output for crime: Murder"
mdl_fit[Crime == "Murder", Summary][[1]]
```


#### Assault

```{r}
#| code-summary: "ANOVA output for crime: Assault"
mdl_fit[Crime == "Assault", Summary][[1]]
```


#### Rape

```{r}
#| code-summary: "ANOVA output for crime: Rape"
mdl_fit[Crime == "Rape", Summary][[1]]
```


:::

### Effect Plot

```{r}
#| code-summary: Boxplot with fitted and true mean
#| fig-asp: 0.5
effect_plot
```

Boxplots displayed arrest rate distributions within each illiteracy group stratified by crime. Points were scattered for detailed visualization, along with fitted means and confidence intervals. Here for all crimes, higher illiteracy corresponds to higher arrest rate and is more visible in murder.

### Post-hoc

```{r}
#| code-summary: Post-hoc plot comparing pairwise difference
#| fig-asp: 0.5
tukey_plot
```

The post-hoc plot has highlighted statistically significant differences between different levels of illiteracy. Here, all pairs of illiteracy levels differ significantly at 95% confidence level for Murder however there is not such significant difference between medium and high illiteracy level for assault and rape.

:::

# Real Data Example

## Data preparation and Dataset

Here, I have used `USArrests` dataset excluding the crime `UrbanPop` and merged it with another dataset `state.x77` using its `Illiteracy` variable for 50 states. The Illiteracy was than categorized using its quantiles into three categories `low`, `medium`, and `high` mimiking the simulation example above.

```{r}
#| code-summary: Merging USArrests and state.x77
arrest <- as_tidytable(USArrests, .keep_rownames = "States") %>% 
  tidytable::left_join(
    as_tidytable(
      state.x77[, "Illiteracy", drop = FALSE],
      .keep_rownames = "States"
    ),
    by = "States"
  ) %>% 
  select(-UrbanPop) %>% 
  mutate(across(Murder:Illiteracy, as.numeric)) %>% 
  mutate(Illiteracy = cut.default(
    Illiteracy,
    breaks = quantile(Illiteracy, c(0, 1/3, 2/3, 1)),
    labels = c("low", "medium", "high"),
    include.lowest = TRUE
  )) %>% 
  mutate(Illiteracy = factor(
    Illiteracy,
    levels = c("high", "medium", "low")
  )) %>% 
  pivot_longer(
    cols = Murder:Rape,
    names_to = "Crime",
    values_to = "Rate"
  ) %>% nest(.by = c(Crime))
```

::: panel-tabset

### Data by Crime

```{r}
#| code-summary: Data by group
arrest
```

### Murder

```{r}
#| code-summary: "Data for crime: Murder"
head(arrest[Crime == "Murder", data][[1]], 3)
```

### Assault
```{r}
#| code-summary: "Data for crime: Assault"
head(arrest[Crime == "Assault", data][[1]], 3)
```

### Rape
```{r}
#| code-summary: "Data for crime: Rape"
head(arrest[Crime == "Rape", data][[1]], 3)
```

:::

## Analysis

I am following a similar pattern as in the analysis of simulated data: distribution plot, fitting an ANOVA model, effect plot showing the fitted value with a boxplot, and a Post-hoc showing pairwise comparison of the effect of illiteracy levels on arrest rate.

::: panel-tabset

```{r}
mdl_fit <- arrest %>% 
  mutate(
    Fit = map(data, ~lm(Rate ~ Illiteracy, data = .x)),
    Summary = map(Fit, summary),
    Anova = map(Fit, anova),
    Tukey = map(Fit, aov) %>% map(TukeyHSD)
  )

mdl_est <- mdl_fit %>% 
  summarize(
    across(Summary, map, broom::tidy, conf.int = TRUE),
    .by = c(Crime)
  ) %>% unnest()

mdl_fit_df <- mdl_fit %>% 
  summarize(
    across(Fit, map, broom::augment),
    .by = c(Crime)
  ) %>% unnest()

eff_df <- mdl_fit %>% 
  summarize(
    across(Fit, map, function(.fit){
      effects::Effect("Illiteracy", .fit) %>%
        as_tidytable()
    }),
    .by = c(Crime)
  ) %>% unnest()

tky_df <- mdl_fit %>% 
  summarize(
    across(Tukey, function(tky) {
      map(tky, purrr::pluck, "Illiteracy") %>%
        map(as_tidytable, .keep_rownames = "terms")
    }),
    .by = "Crime"
  ) %>% unnest()

```

#### Distribution

```{r, out.width="100%", fig.asp=0.5}
ggplot(unnest(arrest), aes(Rate, color = Illiteracy)) +
  geom_density(aes(linetype = "Simulated")) +
  geom_density(
    aes(linetype = "Fitted", x = .fitted),
    data = mdl_fit_df
  ) +
  geom_rug(
    data = eff_df,
    aes(x = fit)
  ) +
  scale_linetype_manual(
    breaks = c("Simulated", "Fitted"),
    values = c("solid", "dashed")
  ) +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  labs(
    x = "Crime",
    y = "Density",
    linetype = NULL
  ) +
  facet_wrap(
    facets = vars(Crime),
    scales = "free"
  )
```

Kernel density alongside normal density curves for each crime shows and validates the normal distribution of the real data and help confirm the normality assumption for ANOVA, ensuring that the real data analysis aligns with the assumptions necessary for valid inference.

#### Fit

::: panel-tabset

##### Murder
```{r}
#| code-summary: "ANOVA output for crime: Murder"
mdl_fit[Crime == "Murder", Summary][[1]]
```

##### Assult
```{r}
#| code-summary: "ANOVA output for crime: Assault"
mdl_fit[Crime == "Assault", Summary][[1]]
```

##### Rape
```{r}
#| code-summary: "ANOVA output for crime: Rape"
mdl_fit[Crime == "Rape", Summary][[1]]
```

:::

#### Effects

```{r, out.width="100%", fig.asp=0.5}
ggplot(unnest(arrest), aes(Rate, Illiteracy)) +
  geom_boxplot(
    notch = FALSE,
    color = "grey",
    outlier.colour = "grey"
  ) +
  geom_point(
    position = position_jitter(height = 0.25),
    color = "grey",
  ) +
  geom_pointrange(
    aes(
      xmin = lower,
      xmax = upper,
      x = fit
    ),
    color = "firebrick",
    data = eff_df
  ) +
  scale_color_brewer(
    name = "Mean",
    palette = "Set1"
  ) +
  theme(
    legend.position = "bottom"
  ) +
  facet_wrap(facets = vars(Crime), scales = "free_x")
```

#### Post-hoc
```{r, out.width="100%", fig.asp=0.5}
ggplot(tky_df, aes(diff, terms)) +
  geom_pointrange(
    aes(xmin = lwr, xmax = upr, x = diff),
    shape = 21,
    fill = "whitesmoke"
  ) +
  geom_vline(
    xintercept = 0,
    linetype = "dashed",
    color = "royalblue"
  ) +
  scale_color_brewer(
    name = "Mean",
    palette = "Set1"
  ) +
  labs(
    y = "Illiteracy",
    x = "Effect difference",
    title = "Pairwise comparison of levels of illiteracy"
  ) +
  expand_limits(x = 0) +
  facet_wrap(facets = vars(Crime), scales = "free_x")
```

:::

The analysis using both simulated and real datasets demonstrates the effectiveness of ANOVA in uncovering patterns. Simulating data that closely mirrors the USArrests dataset provided a controlled environment for testing and understanding variable interactions.

When applied to real data, the analysis confirmed significant differences in arrest rates across illiteracy groups, validating the method. By comparing these results, I highlighted how well-designed simulations can replicate real-world scenarios, offering valuable insights and preparing for real-world analyses.

This approach underscores the utility of combining simulated and real data, showcasing the robustness and reliability of the analytical methods used.